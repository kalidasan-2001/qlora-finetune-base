# qlora_custom.yaml

model_name: "custom_model"
learning_rate: 5e-5
batch_size: 32
num_epochs: 10
max_seq_length: 512
gradient_accumulation_steps: 2
lora_rank: 16
lora_alpha: 32
weight_decay: 0.01
warmup_steps: 500
logging_steps: 100
evaluation_steps: 500
save_steps: 1000
output_dir: "./experiments/output"
seed: 42
fp16: true
use_8bit_adam: true
data:
  train_file: "./data/processed/train.jsonl"
  validation_file: "./data/processed/val.jsonl"
  test_file: "./data/processed/test.jsonl"
  preprocessing:
    - "tokenize"
    - "truncate"
    - "pad"