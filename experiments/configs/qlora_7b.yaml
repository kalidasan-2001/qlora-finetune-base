parameters:
  model_name: "QLoRA-7B"
  learning_rate: 2e-5
  batch_size: 16
  num_train_epochs: 3
  max_seq_length: 512
  gradient_accumulation_steps: 2
  weight_decay: 0.01
  logging_steps: 100
  save_steps: 500
  evaluation_steps: 500
  output_dir: "./output/qlora_7b"
  dataset_path: "./data/processed/train_dataset.json"
  validation_dataset_path: "./data/processed/val_dataset.json"
  tokenizer_name: "facebook/opt-7b"
  lora_rank: 16
  lora_alpha: 32
  lora_dropout: 0.1
  use_fp16: true
  seed: 42

training:
  use_early_stopping: true
  early_stopping_patience: 2
  early_stopping_threshold: 0.01

evaluation:
  metrics:
    - accuracy
    - f1_score
    - perplexity

logging:
  log_level: "INFO"
  log_file: "./logs/qlora_7b.log"